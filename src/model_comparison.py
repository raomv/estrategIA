from fastapi import HTTPException
from pydantic import BaseModel
from llama_index.llms.ollama import Ollama
from typing import Dict, List, Optional
import yaml
import ollama
import pandas as pd
import requests
import traceback

# Importar tu clase RAG y cache_manager
from rag import RAG
from cache_manager import get_cache_manager # Importar get_cache_manager

# Importaciones RAGAS siguiendo la documentaci√≥n oficial
try:
    from ragas.metrics import (
        Faithfulness,
        AnswerRelevancy,
        ContextPrecision,
        # ContextRecall, # ContextRecall tambi√©n necesita ground_truth y puede ser intensivo
    )
    from ragas.llms import LlamaIndexLLMWrapper
    from ragas.integrations.llama_index import evaluate
    from ragas import EvaluationDataset
    RAGAS_AVAILABLE = True
    print("‚úÖ RAGAS importado correctamente con integraci√≥n LlamaIndex")
except ImportError as e:
    print(f"‚ùå RAGAS no est√° disponible: {e}")
    RAGAS_AVAILABLE = False

# Importar LlamaIndex Settings
from llama_index.core.settings import Settings as LlamaSettings
from llama_index.core.evaluation import (
    FaithfulnessEvaluator,
    RelevancyEvaluator,
    CorrectnessEvaluator,
    SemanticSimilarityEvaluator,
    GuidelineEvaluator
)

# Obtener instancia del cache_manager
cache_manager = get_cache_manager()

class CompareRequest(BaseModel):
    message: str  
    models: List[str]
    collection: str
    judge_model: str  # ‚Üê Nuevo campo obligatorio

def get_available_models(config):
    """Obtiene la lista de modelos disponibles en Ollama."""
    try:
        print("üîç Consultando modelos disponibles en Ollama...")
        try:
            models_response = ollama.list()
            print(f"üìã Respuesta de ollama.list(): {models_response}")
            if isinstance(models_response, dict) and 'models' in models_response:
                available_models = [model.get('name') for model in models_response['models'] if model.get('name')]
                print(f"‚úÖ Modelos encontrados: {available_models}")
                return available_models if available_models else [config.get("llm_name", "deepseek-r1:1.5b")]
            else:
                raise Exception("Formato de respuesta inv√°lido de ollama.list()")
        except Exception as ollama_error:
            print(f"‚ùå Error con ollama.list(): {ollama_error}, intentando fallback...")
            response = requests.get(f"{config.get('llm_url', 'http://localhost:11434')}/api/tags", timeout=10)
            response.raise_for_status()
            data = response.json()
            if 'models' in data:
                available_models = [model.get('name') for model in data['models'] if model.get('name')]
                print(f"‚úÖ Modelos encontrados (directo): {available_models}")
                return available_models if available_models else [config.get("llm_name", "deepseek-r1:1.5b")]
            raise Exception("No se encontr√≥ 'models' en la respuesta directa de API")
    except Exception as e:
        print(f"‚ùå Error obteniendo modelos de Ollama: {e}")
        return [config.get("llm_name", "deepseek-r1:1.5b")]


def compare_models(request: CompareRequest, config_file: str, prompt_suffix: str):
    """
    Compara respuestas de m√∫ltiples modelos usando RAGAS oficial.
    Cada modelo seleccionado por el usuario recibe la misma pregunta y RAGAS eval√∫a cada respuesta independientemente,
    utilizando el propio modelo evaluado como juez para sus m√©tricas.
    """
    try:
        print(f"=== INICIANDO COMPARACI√ìN RAGAS OFICIAL ===")
        print(f"Consulta: {request.message[:100]}...")
        
        with open(config_file, "r") as conf:
            config = yaml.safe_load(conf)
        
        # CAMBIO: Solo usar los modelos que el usuario seleccion√≥ expl√≠citamente
        if not request.models or len(request.models) == 0:
            raise ValueError("No se han seleccionado modelos para comparar. El usuario debe seleccionar al menos un modelo desde la interfaz web.")
        
        models_to_compare = request.models
        print(f"Modelos seleccionados por el usuario: {models_to_compare}")
        print(f"RAGAS disponible: {RAGAS_AVAILABLE}")
        
        # Validar que los modelos seleccionados est√©n disponibles en Ollama
        available_models = get_available_models(config)
        invalid_models = [model for model in models_to_compare if model not in available_models]
        if invalid_models:
            print(f"‚ö†Ô∏è Modelos no disponibles en Ollama: {invalid_models}")
            print(f"‚úÖ Modelos disponibles: {available_models}")
            # Filtrar solo los modelos v√°lidos
            models_to_compare = [model for model in models_to_compare if model in available_models]
            if not models_to_compare:
                raise ValueError(f"Ninguno de los modelos seleccionados est√° disponible en Ollama. Modelos disponibles: {available_models}")
        
        print(f"Modelos v√°lidos a comparar: {models_to_compare}")
        
        if not RAGAS_AVAILABLE:
            print("‚ö†Ô∏è RAGAS no disponible.")
            return generate_responses_only(models_to_compare, request.message, config, prompt_suffix)

        # 1. Asegurar y configurar el modelo de embeddings global UNA VEZ.
        print("üîß Configurando modelo de embeddings local...")
        
        # Configurar expl√≠citamente el embed_model ANTES de verificarlo
        cache_manager.ensure_embedding_model_ready(config)
        
        # Obtener el modelo de embeddings del cache manager y asignarlo expl√≠citamente
        embed_model = cache_manager.get_cached_embedding_model()
        if embed_model is None:
            print("‚ö†Ô∏è Modelo de embeddings no encontrado en cache, creando nuevo...")
            embed_model = cache_manager.create_embedding_model(config)
        
        # Configurar LlamaSettings expl√≠citamente 
        LlamaSettings.embed_model = embed_model
        print(f"‚úÖ Modelo de embeddings configurado: {embed_model.model_name}")

        # Ya no se crea un LLM evaluador global aqu√≠. Se crear√° uno por cada modelo a evaluar.
        
        results = {}
        metrics = {}
        
        # CAMBIO: Verificar si LlamaSettings.llm existe antes de acceder
        try:
            original_global_llm = LlamaSettings.llm
        except ValueError:
            # Si no hay LLM configurado, establecer None
            original_global_llm = None
            # Configurar un LLM temporal para evitar errores
            first_model = models_to_compare[0]
            LlamaSettings.llm = Ollama(model=first_model, url=config["llm_url"])
        
        for model_name in models_to_compare:
            try:
                print(f"üìä Iniciando evaluaci√≥n para {model_name} con RAGAS...")
                
                # Configurar el LLM espec√≠fico para este modelo en LlamaSettings y para RAGAS
                specific_llm_base = Ollama(model=model_name, url=config["llm_url"], request_timeout=config.get("llm_request_timeout", 300.0))
                LlamaSettings.llm = specific_llm_base # Establecer como LLM global temporalmente para el query_engine
                print(f"   Temporarily set LlamaSettings.llm to: {model_name}")

                # Crear el LLM wrapper para RAGAS usando el modelo actual
                ragas_llm_for_model = LlamaIndexLLMWrapper(specific_llm_base)
                print(f"   RAGAS LLM for {model_name} created.")

                # Inicializar m√©tricas RAGAS con el LLM espec√≠fico del modelo
                metrics_instances = [
                    Faithfulness(llm=ragas_llm_for_model),
                    AnswerRelevancy(llm=ragas_llm_for_model),
                    ContextPrecision(llm=ragas_llm_for_model),
                ]
                print(f"   M√©tricas RAGAS inicializadas para {model_name}")

                # Crear una instancia de RAG. Usar√° LlamaSettings.embed_model (global) y LlamaSettings.llm (espec√≠fico del modelo).
                temp_rag_config = config.copy() # Para la colecci√≥n correcta
                rag_for_model = RAG(config_file=temp_rag_config, llm=specific_llm_base) # Pasar LLM expl√≠citamente
                
                # qdrant_index() usa el self.embed_model de RAG, que se inicializa desde LlamaSettings o cache_manager
                index_for_model = rag_for_model.qdrant_index() 

                if index_for_model is None:
                    raise ValueError(f"VectorStoreIndex es None para {model_name}. Qdrant podr√≠a no estar accesible o la colecci√≥n no existe.")

                # as_query_engine() usar√° el LlamaSettings.llm y LlamaSettings.embed_model actuales.
                query_engine = index_for_model.as_query_engine(
                    similarity_top_k=config.get("similarity_top_k_ragas", 3), # Config espec√≠fica para RAGAS
                    response_mode=config.get("response_mode_ragas", "tree_summarize")
                )

                if query_engine is None:
                    raise ValueError(f"QueryEngine es None para {model_name}")
                
                # Verificar componentes del query_engine
                if not hasattr(query_engine, '_retriever') or query_engine._retriever is None:
                    raise ValueError(f"Query engine para {model_name} no tiene un retriever v√°lido.")
                if not hasattr(query_engine._retriever, 'retrieve'):
                    raise ValueError(f"Retriever para {model_name} no tiene m√©todo 'retrieve'.")

                print(f"   Query Engine para {model_name} creado. LLM: {query_engine._llm.model if hasattr(query_engine, '_llm') else 'N/A'}. Embeddings: {LlamaSettings.embed_model.model_name}")
                
                print(f"üìã Creando dataset RAGAS para {model_name}...")
                full_question = request.message + prompt_suffix
                ragas_data_list = [{"user_input": full_question, "ground_truth": f"Placeholder GT for {request.message}"}]
                
                dataset = EvaluationDataset.from_dict(ragas_data_list)
                print(f"   Dataset RAGAS creado para {model_name}.")
                
                print(f"üî¨ Ejecutando RAGAS.evaluate para {model_name}...")
                evaluation_result = evaluate(
                    query_engine=query_engine,
                    metrics=metrics_instances,
                    dataset=dataset
                )
                print(f"   Evaluaci√≥n RAGAS completada para {model_name}.")
                
                df = evaluation_result.to_pandas()
                if len(df) > 0:
                    row = df.iloc[0]
                    answer = str(row.get('answer', ''))
                    if not answer: # Si RAGAS no extrajo la respuesta, generarla.
                        print(f"   RAGAS no devolvi√≥ 'answer' para {model_name}, generando manualmente...")
                        response_obj = query_engine.query(full_question)
                        answer = str(response_obj).strip()
                    results[model_name] = answer
                    
                    faithfulness_score = row.get('faithfulness')
                    answer_relevancy_score = row.get('answer_relevancy') 
                    context_precision_score = row.get('context_precision')
                    
                    valid_scores = [s for s in [faithfulness_score, answer_relevancy_score, context_precision_score] if s is not None and not pd.isna(s) and isinstance(s, (float, int))]
                    overall_score = sum(valid_scores) / len(valid_scores) if valid_scores else None
                    
                    metrics[model_name] = {
                        "faithfulness": float(faithfulness_score) if faithfulness_score is not None and not pd.isna(faithfulness_score) else None,
                        "answer_relevancy": float(answer_relevancy_score) if answer_relevancy_score is not None and not pd.isna(answer_relevancy_score) else None,
                        "context_relevancy": float(context_precision_score) if context_precision_score is not None and not pd.isna(context_precision_score) else None, # Mapeado desde ContextPrecision
                        "overall_score": overall_score
                    }
                    print(f"‚úÖ M√©tricas para {model_name}: {metrics[model_name]}")
                else:
                    print(f"‚ö†Ô∏è No se obtuvieron filas de RAGAS para {model_name}")
                    results[model_name] = generate_fallback_response(model_name, request.message, config, prompt_suffix)
                    metrics[model_name] = {"error": "RAGAS no devolvi√≥ resultados en DataFrame"}

            except Exception as e:
                print(f"‚ùå Error evaluando {model_name} con RAGAS: {str(e)}")
                traceback.print_exc()
                results[model_name] = generate_fallback_response(model_name, request.message, config, prompt_suffix)
                metrics[model_name] = {"error": f"Excepci√≥n en RAGAS para {model_name}: {str(e)}"}
        
        LlamaSettings.llm = original_global_llm # Restaurar LLM global
        print("=== COMPARACI√ìN COMPLETADA ===")
        return {"results": results, "metrics": metrics, "models_compared": models_to_compare, "ragas_available": RAGAS_AVAILABLE}
        
    except Exception as e:
        print(f"‚ùå Error general en compare_models: {str(e)}")
        traceback.print_exc()
        # Devolver un error estructurado que el frontend pueda manejar si es necesario
        return {
            "results": {}, 
            "metrics": {}, 
            "models_compared": request.models if request.models else [], 
            "ragas_available": RAGAS_AVAILABLE,
            "error": f"Error general en comparaci√≥n: {str(e)}"
        }

def create_model_query_engine_for_fallback(model_name: str, config: Dict):
    """Crea un query engine simplificado para fallback, asegurando que LlamaSettings est√©n correctas."""
    try:
        # Asegurar que el embed_model est√© configurado
        cache_manager.ensure_embedding_model_ready(config)
        embed_model = cache_manager.get_cached_embedding_model()
        if embed_model is None:
            embed_model = cache_manager.create_embedding_model(config)
        
        # Configurar expl√≠citamente el embed_model
        LlamaSettings.embed_model = embed_model
        
        llm_instance = Ollama(model=model_name, url=config["llm_url"], request_timeout=config.get("llm_request_timeout", 300.0))
        
        # Guardar y restaurar LLM global para esta operaci√≥n espec√≠fica
        original_llm = getattr(LlamaSettings, 'llm', None)
        LlamaSettings.llm = llm_instance
        
        temp_rag_config = config.copy()
        rag_instance = RAG(config_file=temp_rag_config, llm=llm_instance) # RAG usa LlamaSettings
        index_instance = rag_instance.qdrant_index()

        if index_instance is None:
            raise ValueError("Fallback: VectorStoreIndex es None.")
            
        query_engine = index_instance.as_query_engine() # Usa LlamaSettings.llm
        
        LlamaSettings.llm = original_llm # Restaurar
        
        if query_engine is None:
            raise ValueError("Fallback: QueryEngine es None.")
        return query_engine
    except Exception as e:
        print(f"‚ùå Error creando query engine de fallback para {model_name}: {e}")
        return None

def generate_fallback_response(model_name: str, message: str, config: Dict, prompt_suffix: str) -> str:
    print(f"‚ö†Ô∏è Generando respuesta de fallback para {model_name}...")
    try:
        query_engine = create_model_query_engine_for_fallback(model_name, config)
        if query_engine:
            response = query_engine.query(message + prompt_suffix)
            return str(response).strip()
        return f"Error: No se pudo crear query engine de fallback para {model_name}."
    except Exception as e:
        print(f"‚ùå Error generando respuesta de fallback para {model_name}: {e}")
        return f"Error al generar respuesta para {model_name}: {str(e)}"

def generate_responses_only(models_to_compare: List[str], message: str, config: Dict, prompt_suffix: str):
    print("üîÑ Generando solo respuestas (RAGAS no disponible o fall√≥ globalmente)")
    results = {}
    for model_name in models_to_compare:
        results[model_name] = generate_fallback_response(model_name, message, config, prompt_suffix)
    return {
        "results": results, "metrics": {}, "models_compared": models_to_compare,
        "ragas_available": False, "error": "RAGAS no disponible o fall√≥ la evaluaci√≥n."
    }

def academic_llamaindex_evaluation(request: CompareRequest, config: dict):
    """
    Evaluaci√≥n acad√©mica usando LlamaIndex: Juez eval√∫a respuestas, no las genera.
    Basado en metodolog√≠a LLM-as-a-Judge acad√©micamente validada.
    """
    try:
        print("=== EVALUACI√ìN ACAD√âMICA CON LLAMAINDEX ===")
        
        models_to_compare = request.models
        judge_model_name = request.judge_model
        user_question = request.message
        collection_name = request.collection
        
        print(f"üéØ Modelos a evaluar: {models_to_compare}")
        print(f"üë®‚Äç‚öñÔ∏è Modelo juez: {judge_model_name}")
        print(f"üìã Colecci√≥n: {collection_name}")
        print(f"‚ùì Pregunta: {user_question[:100]}...")
        
        # Validar que juez no est√© en modelos a comparar
        if judge_model_name in models_to_compare:
            raise ValueError("El modelo juez no puede estar en la lista de modelos a comparar")
        
        # 1. Configurar embeddings una vez
        cache_manager.ensure_embedding_model_ready(config)
        embed_model = cache_manager.get_cached_embedding_model()
        LlamaSettings.embed_model = embed_model
        
        # 2. Crear el LLM juez (solo para evaluar, no para responder)
        judge_llm = Ollama(model=judge_model_name, url=config["llm_url"], request_timeout=300.0)
        print(f"üèÖ Juez configurado: {judge_model_name}")
        
        # 3. Crear TODOS los evaluadores de LlamaIndex disponibles
        evaluators = {
            "faithfulness": FaithfulnessEvaluator(llm=judge_llm),
            "relevancy": RelevancyEvaluator(llm=judge_llm),
            "correctness": CorrectnessEvaluator(llm=judge_llm),
            "semantic_similarity": SemanticSimilarityEvaluator(llm=judge_llm),
        }
        print(f"üìä Evaluadores creados: {list(evaluators.keys())}")
        
        results = {}
        metrics = {}
        
        # 4. Para cada modelo: generar respuesta y evaluarla con el juez
        for model_name in models_to_compare:
            try:
                print(f"\nüîÑ Procesando modelo: {model_name}")
                
                # Crear RAG para el modelo espec√≠fico
                temp_config = config.copy()
                temp_config["collection_name"] = collection_name
                
                model_llm = Ollama(model=model_name, url=config["llm_url"], request_timeout=300.0)
                LlamaSettings.llm = model_llm
                
                # Crear RAG instance
                from rag import RAG
                rag_instance = RAG(config_file=temp_config, llm=model_llm)
                index = rag_instance.qdrant_index()
                
                if index is None:
                    raise ValueError(f"No se pudo crear √≠ndice para {model_name}")
                
                query_engine = index.as_query_engine(
                    similarity_top_k=config.get("similarity_top_k", 3),
                    response_mode="tree_summarize"
                )
                
                # Generar respuesta del modelo
                response = query_engine.query(user_question + " You can only answer based on the provided context.")
                results[model_name] = str(response).strip()
                print(f"‚úÖ Respuesta generada para {model_name}")
                
                # El juez eval√∫a esta respuesta usando TODAS las m√©tricas
                model_metrics = {}
                for metric_name, evaluator in evaluators.items():
                    try:
                        print(f"   üìä Evaluando {metric_name}...")
                        eval_result = evaluator.evaluate_response(
                            query=user_question, 
                            response=response
                        )
                        
                        model_metrics[metric_name] = {
                            "score": eval_result.score if hasattr(eval_result, 'score') else (1.0 if eval_result.passing else 0.0),
                            "passing": eval_result.passing,
                            "feedback": eval_result.feedback if hasattr(eval_result, 'feedback') else "Evaluaci√≥n completada"
                        }
                        print(f"      ‚úÖ {metric_name}: {model_metrics[metric_name]['score']:.2f}")
                        
                    except Exception as e:
                        print(f"      ‚ùå Error en {metric_name}: {str(e)}")
                        model_metrics[metric_name] = {"error": str(e), "score": 0.0}
                
                # Calcular puntuaci√≥n general
                valid_scores = [m["score"] for m in model_metrics.values() if "score" in m and "error" not in m]
                overall_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
                model_metrics["overall_score"] = overall_score
                
                metrics[model_name] = model_metrics
                print(f"üéØ {model_name} - Puntuaci√≥n general: {overall_score:.2f}")
                
            except Exception as e:
                print(f"‚ùå Error procesando {model_name}: {str(e)}")
                results[model_name] = f"Error: {str(e)}"
                metrics[model_name] = {"error": str(e)}
        
        print("\n=== EVALUACI√ìN ACAD√âMICA COMPLETADA ===")
        
        return {
            "results": results,
            "metrics": metrics,
            "judge_model": judge_model_name,
            "evaluation_method": "LlamaIndex Academic LLM-as-a-Judge",
            "academic_citation": "Liu et al. (2022) - LlamaIndex Framework + LLM-as-a-Judge Methodology"
        }
        
    except Exception as e:
        print(f"‚ùå Error en evaluaci√≥n acad√©mica: {str(e)}")
        return {
            "error": str(e),
            "results": {},
            "metrics": {},
            "judge_model": request.judge_model if hasattr(request, 'judge_model') else "unknown"
        }