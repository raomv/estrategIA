from fastapi import HTTPException
from pydantic import BaseModel
from llama_index.llms.ollama import Ollama
from typing import Dict, List, Optional
import yaml
import ollama
import requests
import json
from typing import List, Dict, Any, Optional
from pydantic import BaseModel
import statistics
import time
from llama_index.llms.ollama import Ollama
from cache_manager import get_cache_manager
from rag import RAG
from ragas_integration import calculate_ragas_metrics  # NUEVO IMPORT

# ‚úÖ A√ëADIR ESTAS DOS L√çNEAS:
import nest_asyncio
nest_asyncio.apply()

# Importar LlamaIndex Settings y Evaluators (SIN RAGAS)
from llama_index.core.settings import Settings as LlamaSettings
from llama_index.core.evaluation import (
    FaithfulnessEvaluator,
    RelevancyEvaluator,
    CorrectnessEvaluator,
    SemanticSimilarityEvaluator,
    GuidelineEvaluator
)

# Obtener instancia del cache_manager
cache_manager = get_cache_manager()

class CompareRequest(BaseModel):
    message: str  
    models: List[str]
    collection: str
    judge_model: str
    include_retrieval_metrics: bool = False
    include_ragas_metrics: bool = False 

def get_available_models(config):
    """Obtiene la lista de modelos disponibles en Ollama."""
    try:
        print("üîç Consultando modelos disponibles en Ollama...")
        try:
            models_response = ollama.list()
            print(f"üìã Respuesta de ollama.list(): {models_response}")
            if isinstance(models_response, dict) and 'models' in models_response:
                available_models = [model.get('name') for model in models_response['models'] if model.get('name')]
                print(f"‚úÖ Modelos encontrados: {available_models}")
                return available_models if available_models else [config.get("llm_name", "deepseek-r1:1.5b")]
            else:
                raise Exception("Formato de respuesta inv√°lido de ollama.list()")
        except Exception as ollama_error:
            print(f"‚ùå Error con ollama.list(): {ollama_error}, intentando fallback...")
            response = requests.get(f"{config.get('llm_url', 'http://localhost:11434')}/api/tags", timeout=10)
            response.raise_for_status()
            data = response.json()
            if 'models' in data:
                available_models = [model.get('name') for model in data['models'] if model.get('name')]
                print(f"‚úÖ Modelos encontrados via fallback: {available_models}")
                return available_models if available_models else [config.get("llm_name", "deepseek-r1:1.5b")]
            else:
                raise Exception("No se pudieron obtener los modelos disponibles")
                
    except Exception as e:
        print(f"‚ùå Error general obteniendo modelos: {e}")
        return [config.get("llm_name", "deepseek-r1:1.5b")]

# Y que la funci√≥n use CompareRequest:
def academic_llamaindex_evaluation(request: CompareRequest, config: dict):
    """
    Evaluaci√≥n acad√©mica usando LlamaIndex: Juez eval√∫a respuestas, no las genera.
    """
    start_time = time.time()
    
    try:
        print("=== EVALUACI√ìN ACAD√âMICA CON LLAMAINDEX NATIVO ===")
        
        models_to_compare = request.models
        judge_model_name = request.judge_model
        user_question = request.message
        collection_name = request.collection
        
        print(f"üéØ Modelos a evaluar: {models_to_compare}")
        print(f"üë®‚Äç‚öñÔ∏è Modelo juez: {judge_model_name}")
        print(f"üìã Colecci√≥n: {collection_name}")
        print(f"‚ùì Pregunta: {user_question[:100]}...")
        
        # Validar que juez no est√© en modelos a comparar
        if judge_model_name in models_to_compare:
            raise ValueError("El modelo juez no puede estar en la lista de modelos a comparar")
        
        # ‚úÖ SOLUCI√ìN: Importar directamente en la funci√≥n
        from cache_manager import get_cache_manager
        cache_manager = get_cache_manager()
        
        # 1. Configurar embeddings una vez
        cache_manager.ensure_embedding_model_ready(config)
        embed_model = cache_manager.get_cached_embedding_model()
        LlamaSettings.embed_model = embed_model
        
        # 2. Crear el LLM juez
        judge_llm = Ollama(model=judge_model_name, url=config["llm_url"], request_timeout=300.0)
        print(f"üèÖ Juez configurado: {judge_model_name}")
        
        # ‚úÖ CREAR EVALUADORES con manejo de errores individual
        evaluators = {}
        
        # Evaluadores que funcionan bien
        try:
            evaluators["faithfulness"] = FaithfulnessEvaluator(llm=judge_llm)  # ‚úÖ SIN prompt_template
            print("‚úÖ FaithfulnessEvaluator creado")
        except Exception as e:
            print(f"‚ùå Error creando FaithfulnessEvaluator: {e}")
        
        try:
            evaluators["relevancy"] = RelevancyEvaluator(llm=judge_llm)  # ‚úÖ SIN prompt_template
            print("‚úÖ RelevancyEvaluator creado")
        except Exception as e:
            print(f"‚ùå Error creando RelevancyEvaluator: {e}")
        
        try:
            evaluators["correctness"] = CorrectnessEvaluator(llm=judge_llm)  # ‚úÖ SIN prompt_template
            print("‚úÖ CorrectnessEvaluator creado")
        except Exception as e:
            print(f"‚ö†Ô∏è CorrectnessEvaluator no disponible: {e}")
        
        # A√±adir SemanticSimilarityEvaluator (no requiere LLM)
        try:
            evaluators["semantic_similarity"] = SemanticSimilarityEvaluator()
            print("‚úÖ SemanticSimilarityEvaluator creado")
        except Exception as e:
            print(f"‚ùå Error creando SemanticSimilarityEvaluator: {e}")
        
        # A√±adir GuidelineEvaluator (con guidelines espec√≠ficas)
        try:
            evaluators["guideline"] = GuidelineEvaluator(
                llm=judge_llm,
                guidelines="IGNORE your training knowledge. Evaluate ONLY against the provided context. The response should use only information from the given context documents. Do not apply external knowledge about typical military structures or common practices."
            )
            print("‚úÖ GuidelineEvaluator creado (con instrucciones estrictas)")
        except Exception as e:
            print(f"‚ö†Ô∏è GuidelineEvaluator no disponible: {e}")
        
        print(f"üìä Evaluadores disponibles: {list(evaluators.keys())}")
        
        if not evaluators:
            return {"error": "No se pudieron crear evaluadores", "results": {}, "metrics": {}}
        
        # 3. Conectar al √≠ndice existente
        print(f"üîç Conectando a colecci√≥n existente: {collection_name}")
        
        temp_config = config.copy()
        temp_config["collection_name"] = collection_name
        
        initial_llm = Ollama(model=models_to_compare[0], url=config["llm_url"], request_timeout=300.0)
        rag_instance = RAG(config_file=temp_config, llm=initial_llm)
        shared_index = rag_instance.qdrant_index()
        
        if shared_index is None:
            raise ValueError(f"No se pudo conectar a la colecci√≥n {collection_name}")
        
        print(f"‚úÖ Conectado a √≠ndice existente: {collection_name}")
        
        # ‚úÖ EXTRAER CONTEXTOS UNA VEZ ANTES DEL BUCLE (para RAGAS)
        print(f"üîç Extrayendo contextos de Qdrant para RAGAS...")
        
        # Usar el primer modelo solo para extraer contextos (sin almacenar respuesta)
        temp_llm = Ollama(model=models_to_compare[0], url=config["llm_url"], request_timeout=300.0)
        temp_query_engine = shared_index.as_query_engine(
            llm=temp_llm,
            similarity_top_k=config.get("similarity_top_k", 3),
            response_mode="tree_summarize"
        )
        
        # Query solo para extraer contextos
        temp_response = temp_query_engine.query(user_question)
        shared_retrieved_contexts = []
        
        if hasattr(temp_response, 'source_nodes') and temp_response.source_nodes:
            for node in temp_response.source_nodes:
                shared_retrieved_contexts.append(node.text)
            print(f"‚úÖ Contextos extra√≠dos para RAGAS: {len(shared_retrieved_contexts)} fragmentos")
            # Debug: mostrar preview de contextos
            for i, ctx in enumerate(shared_retrieved_contexts[:2]):
                preview = ctx[:150] + "..." if len(ctx) > 150 else ctx
                print(f"   üìÑ Context {i+1}: {preview}")
        else:
            print(f"‚ö†Ô∏è WARNING: No se recuperaron contexts de Qdrant para RAGAS")
            shared_retrieved_contexts = []
        
        results = {}
        metrics = {}
        
        # ‚úÖ VARIABLE PARA QUERY_ENGINE (para retrieval metrics)
        query_engine = None
        
        # 4. Para cada modelo: evaluar con todos los evaluadores disponibles
        for model_name in models_to_compare:
            print(f"\nüîÑ Procesando modelo: {model_name}")
            
            # ‚úÖ INICIALIZAR model_metrics AQU√ç
            model_metrics = {}
            
            try:
                model_llm = Ollama(model=model_name, url=config["llm_url"], request_timeout=300.0)
                
                query_engine = shared_index.as_query_engine(
                    llm=model_llm,
                    similarity_top_k=config.get("similarity_top_k", 3),
                    response_mode="tree_summarize"
                )
                
                full_question = user_question + " You can only answer based on the provided context."
                response = query_engine.query(full_question)
                response_text = str(response).strip()
                
                print(f"   üìù Respuesta generada ({len(response_text)} chars): {response_text[:100]}...")
                results[model_name] = response_text
                
                # ‚úÖ MANTENER extracci√≥n de contexts para debugging
                retrieved_contexts = []
                if hasattr(response, 'source_nodes') and response.source_nodes:
                    for node in response.source_nodes:
                        retrieved_contexts.append(node.text)
                    print(f"      üìÑ Contexts recuperados del RAG: {len(retrieved_contexts)} fragmentos")
                else:
                    print(f"      ‚ö†Ô∏è WARNING: No se recuperaron contexts de Qdrant")

                # ‚úÖ EVALUACI√ìN CON DEBUGGING COMPLETO Y MANEJO DE ERRORES JSON
                for metric_name, evaluator in evaluators.items():
                    try:
                        print(f"   üìä Evaluando {metric_name}...")
                        
                        if metric_name == "semantic_similarity":
                            eval_result = evaluator.evaluate_response(
                                response=response,
                                reference=user_question
                            )
                        elif metric_name == "guideline":
                            try:
                                # ‚úÖ GuidelineEvaluator con manejo espec√≠fico de errores JSON
                                eval_result = evaluator.evaluate_response(
                                    query=user_question,
                                    response=response
                                )
                                
                                # Extraer score de manera robusta
                                if hasattr(eval_result, 'passing') and eval_result.passing is not None:
                                    score = 1.0 if eval_result.passing else 0.0
                                elif hasattr(eval_result, 'score') and eval_result.score is not None:
                                    score = float(eval_result.score)
                                else:
                                    print(f"      ‚ö†Ô∏è Guideline evaluation sin score v√°lido, usando 0.5")
                                    score = 0.5
                                
                                model_metrics[metric_name] = {
                                    "score": round(score, 3),
                                    "feedback": getattr(eval_result, 'feedback', 'No feedback')
                                }
                                print(f"      ‚úÖ {metric_name}: {score}")
                                continue  # Saltar el resto del procesamiento
                                
                            except ValueError as json_error:
                                error_msg = str(json_error)
                                if "Could not extract json string" in error_msg:
                                    print(f"      ‚ö†Ô∏è {metric_name} - Error de parsing JSON del modelo juez")
                                    # Intentar extraer informaci√≥n del texto plano
                                    if "Passing: false" in error_msg:
                                        score = 0.0
                                        print(f"      ‚úÖ Extra√≠do de texto: {metric_name} = 0.0")
                                    elif "Passing: true" in error_msg:
                                        score = 1.0
                                        print(f"      ‚úÖ Extra√≠do de texto: {metric_name} = 1.0")
                                    else:
                                        score = 0.5  # Score neutro
                                        print(f"      ‚ö†Ô∏è No se pudo extraer score, usando neutro: 0.5")
                                    
                                    model_metrics[metric_name] = {
                                        "score": score,
                                        "feedback": f"JSON parsing error, extracted from text: {error_msg[:200]}"
                                    }
                                    continue
                                else:
                                    raise json_error
                        else:
                            # faithfulness, relevancy, correctness
                            eval_result = evaluator.evaluate_response(
                                query=user_question,
                                response=response
                            )
                        
                        # ‚úÖ RESTO DEL PROCESAMIENTO (C√ìDIGO EXISTENTE)
                        score = None
                        if hasattr(eval_result, 'score'):
                            raw_score = eval_result.score
                            print(f"      üìè Score raw: {raw_score} (tipo: {type(raw_score)})")
                            if raw_score is not None:
                                score = float(raw_score)
                                print(f"      üìè Score convertido: {score}")
                            else:
                                print(f"      ‚ö†Ô∏è Score es None para {metric_name}")
                        else:
                            print(f"      ‚ùå No tiene atributo 'score': {metric_name}")
                        
                        # Extraer passing
                        passing = None
                        if hasattr(eval_result, 'passing'):
                            passing = eval_result.passing
                            print(f"      ‚úÖ Passing: {passing} (tipo: {type(passing)})")
                        
                        # Extraer feedback
                        feedback = ""
                        if hasattr(eval_result, 'feedback'):
                            raw_feedback = eval_result.feedback
                            if raw_feedback:
                                feedback = str(raw_feedback)
                                print(f"      üí¨ Feedback completo: {feedback}")  # ‚úÖ CAMBIO 1: SIN CORTE
                                
                                # ‚úÖ PARA CORRECTNESS: Extraer score del feedback CON NORMALIZACI√ìN CORRECTA
                                if metric_name == "correctness" and score is None:
                                    import re
                                    
                                    # ‚úÖ BUSCAR PRIMERO DENTRO DE <think> TAGS
                                    think_content = ""
                                    think_match = re.search(r'<think>(.*?)</think>', feedback, flags=re.DOTALL)
                                    if think_match:
                                        think_content = think_match.group(1)
                                        print(f"      üß† Contenido <think>: {think_content}")
                                    
                                    score_patterns = [
                                        r'\b(\d+\.?\d*)\s*(?:out of|/)\s*5',  # "4.0 out of 5"
                                        r'\bscore[:\s]*(\d+\.?\d*)',          # "score: 4.0"
                                        r'\b(\d+\.?\d*)\s*/\s*5',             # "4.0/5"
                                        r'\b(\d+\.?\d*)\b'                    # cualquier n√∫mero
                                    ]
                                    
                                    # ‚úÖ BUSCAR PRIMERO EN <think>, LUEGO EN FEEDBACK COMPLETO
                                    search_areas = [think_content, feedback] if think_content else [feedback]
                                    
                                    for search_text in search_areas:
                                        for pattern in score_patterns:
                                            score_match = re.search(pattern, search_text, re.IGNORECASE)
                                            if score_match:
                                                try:
                                                    extracted_score = float(score_match.group(1))
                                                    print(f"      üîß Score extra√≠do de {'<think>' if search_text == think_content else 'feedback'}: {extracted_score}")
                                                    
                                                    # ‚úÖ NORMALIZACI√ìN CORRECTA SIN REDONDEO AGRESIVO
                                                    if extracted_score > 1:
                                                        # Asumimos escala de 5 puntos
                                                        score = extracted_score / 5.0
                                                        print(f"      üîß Score normalizado (escala 5): {score:.3f}")
                                                    else:
                                                        # Ya est√° en escala 0-1
                                                        score = extracted_score
                                                        print(f"      üîß Score ya normalizado: {score:.3f}")
                                                    
                                                    # ‚úÖ L√çMITE M√ÅXIMO SIN REDONDEO AGRESIVO
                                                    score = min(score, 1.0)
                                                    break
                                                except ValueError:
                                                    continue
                                        if score is not None:
                                            break
                        
                        # ‚úÖ CONVERSI√ìN FINAL CON NORMALIZACI√ìN UNIVERSAL
                        if score is None and passing is not None:
                            score = 1.0 if passing else 0.0
                            print(f"      üîÑ Score convertido desde passing: {score}")
                        elif score is None:
                            score = 0.0
                            print(f"      ‚ö†Ô∏è Score por defecto para {metric_name}: {score}")
                        
                        # ‚úÖ NORMALIZACI√ìN A ESCALA 0-1 PARA TODOS
                        if score > 1.0:
                            # Si el score viene en escala 0-5 o 0-10, normalizar
                            if score <= 5.0:
                                score = score / 5.0  # Escala 0-5 ‚Üí 0-1
                                print(f"      üîß Score normalizado (escala 5): {score:.3f}")
                            elif score <= 10.0:
                                score = score / 10.0  # Escala 0-10 ‚Üí 0-1
                                print(f"      üîß Score normalizado (escala 10): {score:.3f}")
                            else:
                                score = 1.0  # Clamp a m√°ximo 1.0
                                print(f"      ‚ö†Ô∏è Score clampeado a 1.0: {score}")
                        
                        # ‚úÖ GARANTIZAR RANGO 0-1
                        score = max(0.0, min(1.0, score))
                        final_score = round(float(score), 3)
                        
                        # ‚úÖ CAMBIO 2: ELIMINAR 'passing' DEL JSON AL FRONTEND
                        model_metrics[metric_name] = {
                            "score": final_score,
                            "feedback": feedback  # ‚úÖ SIN CORTE [:300]
                        }
                        
                        print(f"      ‚úÖ {metric_name}: {final_score}")
                        
                    except Exception as e:
                        print(f"      ‚ùå Error evaluando {metric_name}: {e}")
                        import traceback
                        traceback.print_exc()
                        model_metrics[metric_name] = {
                            "score": 0.0,
                            "feedback": f"Error en evaluaci√≥n: {str(e)}"
                        }
                
                # ‚úÖ CALCULAR PUNTUACI√ìN GENERAL CORRECTAMENTE
                valid_scores = []
                for metric_name, metric_data in model_metrics.items():
                    if isinstance(metric_data, dict) and 'score' in metric_data:
                        score_value = metric_data['score']
                        if isinstance(score_value, (int, float)) and score_value >= 0:
                            valid_scores.append(score_value)
                
                overall_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0
                model_metrics["overall_score"] = round(overall_score, 2)
                
                print(f"üéØ {model_name} - Puntuaci√≥n general: {overall_score:.2f} (de {len(valid_scores)} m√©tricas v√°lidas)")
                
            except Exception as e:
                print(f"‚ùå Error procesando {model_name}: {str(e)}")
                results[model_name] = f"Error: {str(e)}"
                model_metrics = {"error": str(e)}
            
            # ‚úÖ ASEGURAR QUE model_metrics SIEMPRE SE ASIGNA
            metrics[model_name] = model_metrics
        
        print("\n=== EVALUACI√ìN ACAD√âMICA COMPLETADA ===")
        print(f"üìä Contexto: Colecci√≥n con pocos documentos puede resultar en scores altos")
        
        # ‚úÖ EVALUACI√ìN DE RETRIEVAL CORREGIDA
        retrieval_metrics = None
        if hasattr(request, 'include_retrieval_metrics') and request.include_retrieval_metrics and query_engine:
            print(f"\nüîç === INICIANDO EVALUACI√ìN DE RETRIEVAL ===")
            retrieval_metrics = evaluate_retrieval_metrics(
                query_engine=query_engine,
                user_query=request.message,
                config=config
            )
            print(f"üîç === EVALUACI√ìN DE RETRIEVAL COMPLETADA ===\n")
        
        # ‚úÖ DESPU√âS DE EVALUAR TODOS LOS MODELOS CON M√âTRICAS NATIVAS
        # GENERAR RESPUESTA DE REFERENCIA DEL JUEZ PARA RAGAS
        judge_reference_response = None
        
        if config.get("include_ragas", True):
            print(f"\nü§ñ === GENERANDO RESPUESTA DE REFERENCIA DEL JUEZ PARA RAGAS ===")
            
            try:
                # ‚úÖ USAR EL MISMO PATR√ìN QUE LOS OTROS MODELOS EN EL PROYECTO
                from index_manager import IndexManager
                from cache_manager import get_cache_manager
                
                # Obtener cache manager y collection
                cache_manager = get_cache_manager()
                embed_model = cache_manager.get_cached_embedding_model()
                
                # Crear index manager para el juez
                index_manager = IndexManager(embed_model=embed_model)
                
                print(f"üîç Configurando query engine del juez con colecci√≥n: {request.collection}")
                
                # ‚úÖ CREAR QUERY ENGINE DEL JUEZ IGUAL QUE LOS OTROS MODELOS
                judge_query_engine = index_manager.get_query_engine(
                    collection_name=request.collection,
                    llm=judge_llm,
                    similarity_top_k=config.get("similarity_top_k", 3),
                    response_mode="compact"
                )
                
                print(f"ü§ñ Juez generando respuesta de referencia para: '{user_question}'")
                print(f"üìÑ Usando misma colecci√≥n y configuraci√≥n que otros modelos")
                
                # ‚úÖ EL JUEZ GENERA SU RESPUESTA USANDO EL MISMO CONTEXTO
                judge_response_obj = judge_query_engine.query(user_question)
                judge_reference_response = str(judge_response_obj).strip()
                
                print(f"‚úÖ Respuesta del juez generada:")
                print(f"   üìè Longitud: {len(judge_reference_response)} caracteres")
                print(f"   üìù Preview: {judge_reference_response[:300]}...")
                
                # ‚úÖ VERIFICAR QUE LA RESPUESTA ES DIFERENTE DE LOS MODELOS
                is_different = True
                for model_name, model_response in results.items():
                    if judge_reference_response.strip() == model_response.strip():
                        print(f"‚ö†Ô∏è Respuesta del juez id√©ntica a {model_name}")
                        is_different = False
                        break
                
                if not is_different or len(judge_reference_response) < 50:
                    print(f"‚ö†Ô∏è Generando respuesta alternativa del juez con prompt espec√≠fico...")
                    
                    # Prompt espec√≠fico para generar respuesta diferente
                    judge_prompt = f"""You are an expert evaluator. Based on your knowledge and the context provided, provide a comprehensive, authoritative answer to this question: {user_question}

Your answer should be:
- Detailed and well-structured
- Based on authoritative knowledge  
- Different from typical model responses
- Comprehensive and educational

Question: {user_question}"""
                    
                    judge_direct_response = judge_llm.complete(judge_prompt)
                    judge_reference_response = str(judge_direct_response).strip()
                    
                    print(f"‚úÖ Respuesta alternativa del juez: {len(judge_reference_response)} chars")
                    print(f"üìù Preview alternativo: {judge_reference_response[:300]}...")
                
            except Exception as judge_gen_error:
                print(f"‚ùå Error generando respuesta del juez: {judge_gen_error}")
                import traceback
                print(f"Traceback: {traceback.format_exc()[:400]}...")
                judge_reference_response = None
        
        # ‚úÖ SOLO LLAMAR RAGAS SI HAY RESPUESTA DEL JUEZ V√ÅLIDA Y DIFERENTE
        if config.get("include_ragas", True) and judge_reference_response and len(judge_reference_response.strip()) > 50:
            print(f"\nüéØ === CALCULANDO M√âTRICAS RAGAS ===")
            print(f"üéØ Contextos disponibles para RAGAS: {len(shared_retrieved_contexts)} fragmentos")
            print(f"üéØ RAGAS usando modelo juez: {request.judge_model}")
            print(f"üéØ Respuesta de referencia del juez: {len(judge_reference_response)} chars")
            print(f"üìù Preview referencia: {judge_reference_response[:200]}...")
            
            # ‚úÖ VERIFICAR QUE ES DIFERENTE DE CADA MODELO ANTES DE RAGAS
            different_from_all = True
            for model_name, model_response in results.items():
                similarity_ratio = len(set(judge_reference_response.split()) & set(model_response.split())) / max(len(judge_reference_response.split()), len(model_response.split()))
                print(f"   üìä Similitud con {model_name}: {similarity_ratio:.3f}")
                if similarity_ratio > 0.9:  # Si m√°s del 90% de palabras son iguales
                    print(f"   ‚ö†Ô∏è Judge response muy similar a {model_name}")
                    different_from_all = False
            
            if different_from_all:
                try:
                    ragas_metrics = calculate_ragas_metrics(
                        user_query=user_question,
                        model_responses=results,  # ‚Üê Respuestas de todos los modelos
                        contexts=shared_retrieved_contexts,  # ‚Üê Mismo contexto usado por todos
                        judge_response=judge_reference_response,  # ‚Üê Respuesta REAL del juez
                        config={
                            "judge_model": request.judge_model,
                            "llm_url": config["llm_url"]
                        }
                    )
                    
                    if ragas_metrics:
                        print(f"‚úÖ M√©tricas RAGAS calculadas para {len(ragas_metrics)} modelos")
                        
                        # ‚úÖ INTEGRAR M√âTRICAS RAGAS CON EL RESTO
                        for model_name, ragas_scores in ragas_metrics.items():
                            if model_name in metrics:
                                metrics[model_name].update(ragas_scores)
                                print(f"   üìä {model_name}: {len(ragas_scores)} m√©tricas RAGAS a√±adidas")
                    else:
                        print(f"‚ö†Ô∏è No se obtuvieron m√©tricas RAGAS")
                        
                except Exception as ragas_error:
                    print(f"‚ùå Error calculando m√©tricas RAGAS: {ragas_error}")
                    import traceback
                    print(f"Traceback RAGAS: {traceback.format_exc()[:400]}...")
            else:
                print(f"‚ö†Ô∏è RAGAS omitido - respuesta del juez muy similar a modelos evaluados")
                
        elif config.get("include_ragas", True):
            print(f"‚ö†Ô∏è RAGAS omitido - no hay respuesta v√°lida del juez")

        return {
            "results": results,
            "metrics": metrics,
            "retrieval_metrics": retrieval_metrics,
            "metadata": {
                "judge_model": request.judge_model,
                "total_models": len(models_to_compare),
                "collection": request.collection,
                "evaluation_time": time.time() - start_time,
                "retrieval_evaluated": retrieval_metrics is not None
            }
        }
        
    except Exception as e:
        print(f"‚ùå Error en evaluaci√≥n acad√©mica: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "error": str(e),
            "results": {},
            "metrics": {},
            "retrieval_metrics": None,
            "metadata": {
                "judge_model": request.judge_model if hasattr(request, 'judge_model') else "unknown",
                "evaluation_time": time.time() - start_time if 'start_time' in locals() else 0
            }
        }

def evaluate_retrieval_metrics(query_engine, user_query, config):
    """Eval√∫a retrieval con m√©tricas label-free adaptado al proyecto"""
    import time
    import numpy as np
    from itertools import combinations
    
    try:
        print("üîç === EVALUACI√ìN DE RETRIEVAL (LABEL-FREE) ===")
        
        # ‚úÖ RETRIEVAL con timing
        retriever = query_engine.retriever
        t0 = time.time()
        nodes = retriever.retrieve(user_query)
        t1 = time.time()
        retrieval_time_ms = round((t1 - t0) * 1000.0, 2)
        
        k = len(nodes)
        print(f"üìÑ Documentos recuperados: {k} en {retrieval_time_ms}ms")
        
        if k == 0:
            return {
                "query": user_query,
                "retrieved_count": 0,
                "retrieval_time_ms": retrieval_time_ms,
                "error": "No se recuperaron documentos"
            }
        
        # ‚úÖ EXTRAER SCORES (compatibility con tu setup)
        scores = []
        for i, node in enumerate(nodes):
            score = getattr(node, 'score', 0.0)
            print(f"   üìÑ Doc {i+1}: score={score:.4f}")
            scores.append(float(score))
        
        # ‚úÖ M√âTRICAS BASADAS EN SCORES
        scores_sorted = sorted(scores, reverse=True)
        score_at_1 = scores_sorted[0]
        mean_score = float(np.mean(scores))
        var_score = float(np.var(scores)) if k > 1 else 0.0
        margin_at_1 = float(scores_sorted[0] - scores_sorted[1]) if k > 1 else score_at_1
        
        threshold = float(config.get("similarity_threshold", 0.7))
        accept_rate = float(np.mean([s >= threshold for s in scores]))
        
        print(f"üìä Score@1: {score_at_1:.4f}")
        print(f"üìä Mean Score: {mean_score:.4f}")
        print(f"üìä Accept Rate@{threshold}: {accept_rate:.4f}")
        
        # ‚úÖ M√âTRICAS DE EMBEDDINGS (adaptado a tu proyecto)
        try:
            # Acceder al embed_model desde el query_engine
            embed_model = None
            if hasattr(query_engine, '_service_context') and hasattr(query_engine._service_context, 'embed_model'):
                embed_model = query_engine._service_context.embed_model
            elif hasattr(query_engine, 'retriever') and hasattr(query_engine.retriever, '_embed_model'):
                embed_model = query_engine.retriever._embed_model
            
            if embed_model:
                # Query embedding
                q_emb = np.array(embed_model.get_query_embedding(user_query), dtype=np.float32)
                
                # Doc embeddings - intentar diferentes m√©todos
                doc_embs = []
                for node in nodes:
                    try:
                        # M√©todo 1: embedding directo en node
                        if hasattr(node, 'embedding') and node.embedding is not None:
                            doc_embs.append(np.array(node.embedding, dtype=np.float32))
                        # M√©todo 2: desde node.node
                        elif hasattr(node, 'node') and hasattr(node.node, 'embedding') and node.node.embedding:
                            doc_embs.append(np.array(node.node.embedding, dtype=np.float32))
                        # M√©todo 3: regenerar embedding del texto
                        else:
                            text = getattr(node, 'text', getattr(node, 'content', ''))
                            if hasattr(node, 'node'):
                                text = getattr(node.node, 'text', text)
                            if text:
                                emb = embed_model.get_text_embedding(text)
                                doc_embs.append(np.array(emb, dtype=np.float32))
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error obteniendo embedding para doc: {e}")
                        continue
                
                if doc_embs and len(doc_embs) == k:
                    # Query-Doc similarities
                    qd_sims = []
                    for doc_emb in doc_embs:
                        sim = np.dot(q_emb, doc_emb) / (np.linalg.norm(q_emb) * np.linalg.norm(doc_emb))
                        qd_sims.append(float(sim))
                    
                    qd_mean = float(np.mean(qd_sims))
                    qd_max = float(np.max(qd_sims))
                    
                    # Doc-Doc coherence
                    if k > 1:
                        dd_sims = []
                        for i in range(k):
                            for j in range(i+1, k):
                                sim = np.dot(doc_embs[i], doc_embs[j]) / (
                                    np.linalg.norm(doc_embs[i]) * np.linalg.norm(doc_embs[j])
                                )
                                dd_sims.append(float(sim))
                        docdoc_coherence = float(np.mean(dd_sims))
                        diversity = float(1.0 - docdoc_coherence)
                    else:
                        docdoc_coherence = 1.0
                        diversity = 0.0
                    
                    print(f"üìä Query-Doc Mean: {qd_mean:.4f}")
                    print(f"üìä Doc-Doc Coherence: {docdoc_coherence:.4f}")
                    print(f"üìä Diversity: {diversity:.4f}")
                    
                else:
                    print("‚ö†Ô∏è No se pudieron obtener embeddings para m√©tricas geom√©tricas")
                    qd_mean = qd_max = docdoc_coherence = diversity = None
            else:
                print("‚ö†Ô∏è No se pudo acceder al embed_model")
                qd_mean = qd_max = docdoc_coherence = diversity = None
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error calculando m√©tricas de embeddings: {e}")
            qd_mean = qd_max = docdoc_coherence = diversity = None
        
        # ‚úÖ M√âTRICAS OPERACIONALES
        unique_sources = len(set(
            getattr(node, 'doc_id', getattr(getattr(node, 'node', node), 'doc_id', f'doc_{i}'))
            for i, node in enumerate(nodes)
        ))
        
        result = {
            "query": user_query,
            "retrieved_count": k,
            "retrieval_time_ms": retrieval_time_ms,
            "score_at_1": round(score_at_1, 4),
            "mean_score": round(mean_score, 4),
            "var_score": round(var_score, 6),
            "margin_at_1": round(margin_at_1, 4),
            "accept_rate_at_threshold": round(accept_rate, 4),
            "threshold_used": threshold,
            "unique_sources": unique_sources,
            "metadata": {
                "embedding_model": config.get("embedding_model", "fastembed"),
                "vector_store": "qdrant", 
                "evaluation_timestamp": time.time(),
                "similarity_top_k": config.get("similarity_top_k", 3)
            }
        }
        
        # A√±adir m√©tricas de embeddings si est√°n disponibles
        if qd_mean is not None:
            result.update({
                "qd_mean": round(qd_mean, 4),
                "qd_max": round(qd_max, 4),
                "docdoc_coherence": round(docdoc_coherence, 4),
                "diversity": round(diversity, 4)
            })
        
        return result
        
    except Exception as e:
        print(f"‚ùå Error en evaluate_retrieval_metrics: {e}")
        return {
            "query": user_query,
            "error": str(e),
            "retrieved_count": 0
        }