"""
Integraci√≥n de m√©tricas RAGAS para evaluaci√≥n est√°ndar.
RAGAS 0.2.0 con LlamaIndex (sin LangChain)
"""
import logging
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

def calculate_ragas_metrics(user_query, model_responses, contexts, judge_response, config=None):
    """
    Calcula m√©tricas RAGAS usando el MISMO modelo juez seleccionado por el usuario
    RAGAS 0.2.0 + LlamaIndex (sin LangChain)
    """
    try:
        # ‚úÖ USAR EL MISMO PATR√ìN QUE LAS M√âTRICAS NATIVAS
        judge_model_name = config.get("judge_model") if config else None
        llm_url = config.get("llm_url") if config else "http://localhost:11434"
        
        if not judge_model_name:
            print("‚ùå No se proporcion√≥ modelo juez para RAGAS")
            return {}
        
        print(f"üéØ RAGAS usando modelo juez seleccionado: {judge_model_name}")
        
        # ‚úÖ IMPORTS CORRECTOS PARA RAGAS 0.2.0 CON LLAMAINDEX
        try:
            from ragas.llms.base import LlamaIndexLLMWrapper
            from ragas.embeddings.base import LlamaIndexEmbeddingsWrapper
            print("‚úÖ Imports correctos desde .base funcionando")
        except ImportError as e:
            print(f"‚ùå No se pudo importar desde .base: {e}")
            return {}
        
        # ‚úÖ CREAR EL MISMO LLM QUE USAN LAS M√âTRICAS NATIVAS
        from llama_index.llms.ollama import Ollama
        judge_llm = Ollama(model=judge_model_name, base_url=llm_url, request_timeout=600.0)
        
        # ‚úÖ CONFIGURAR RAGAS CON ESE LLM (SOLO LLAMAINDEX)
        from ragas import evaluate
        from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall
        
        # Usar los wrappers correctos
        ragas_llm = LlamaIndexLLMWrapper(llm=judge_llm)
        
        # ‚úÖ USAR EL MISMO EMBED_MODEL QUE EL PROYECTO
        from cache_manager import get_cache_manager
        cache_manager = get_cache_manager()
        embed_model = cache_manager.get_cached_embedding_model()
        ragas_embeddings = LlamaIndexEmbeddingsWrapper(embeddings=embed_model)
        
        # Configurar m√©tricas RAGAS
        faithfulness.llm = ragas_llm
        answer_relevancy.llm = ragas_llm
        answer_relevancy.embeddings = ragas_embeddings
        context_precision.llm = ragas_llm
        context_recall.llm = ragas_llm
        context_recall.embeddings = ragas_embeddings
        
        print(f"‚úÖ RAGAS configurado con juez LlamaIndex: {judge_model_name}")
        
        # Calcular m√©tricas
        ragas_results = {}
        
        for model_name, response_text in model_responses.items():
            try:
                from datasets import Dataset
                
                # ‚úÖ DEBUG COMPLETO - Ver exactamente qu√© recibe RAGAS
                print(f"\nüîç === DEBUG COMPLETO PARA {model_name} ===")
                print(f"üìù User query ({len(user_query)} chars): {user_query}")
                print(f"üìù Model response ({len(response_text)} chars): {response_text[:200]}...")
                print(f"üìù Judge response ({len(str(judge_response)) if judge_response else 0} chars): {str(judge_response)[:200] if judge_response else 'NONE'}...")
                print(f"üìù Contexts count: {len(contexts)}")
                
                if contexts:
                    for i, ctx in enumerate(contexts[:3]):  # Mostrar primeros 3 contextos
                        print(f"   üìÑ Context {i+1} ({len(ctx)} chars): {ctx[:150]}...")
                else:
                    print("   ‚ùå NO HAY CONTEXTOS")
                
                # ‚úÖ VERIFICAR CALIDAD DE DATOS
                if not user_query or len(user_query.strip()) < 5:
                    print(f"‚ùå PROBLEMA: User query muy corto o vac√≠o")
                
                if not response_text or len(response_text.strip()) < 10:
                    print(f"‚ùå PROBLEMA: Response muy corta o vac√≠a")
                
                if not judge_response or len(str(judge_response).strip()) < 10:
                    print(f"‚ùå PROBLEMA: Judge response muy corto o vac√≠o: '{judge_response}'")
                
                if not contexts or len(contexts) == 0:
                    print(f"‚ùå PROBLEMA: No hay contextos")
                
                # ‚úÖ CREAR DATASET CON VERIFICACI√ìN
                data = {
                    "question": [user_query],
                    "answer": [response_text],
                    "contexts": [contexts if contexts else ["No context available"]],
                    "ground_truth": [str(judge_response)] if judge_response else [response_text]  # Fallback
                }
                
                print(f"üìä Dataset creado:")
                print(f"   question: '{data['question'][0]}'")
                print(f"   answer: '{data['answer'][0][:100]}...'")
                print(f"   contexts: {len(data['contexts'][0])} items")
                print(f"   ground_truth: '{data['ground_truth'][0][:100]}...'")
                
                dataset = Dataset.from_dict(data)
                print(f"‚úÖ Dataset HuggingFace creado correctamente")
                
                # ‚úÖ EVALUAR CON DEBUG DETALLADO
                print(f"üîÑ Iniciando evaluaci√≥n RAGAS...")
                
                try:
                    result = evaluate(
                        dataset=dataset,
                        metrics=[faithfulness, answer_relevancy, context_precision, context_recall]
                    )
                    print(f"‚úÖ Evaluaci√≥n RAGAS completada")
                    
                    # ‚úÖ DEBUG: Inspeccionar resultado crudo
                    print(f"üîç Resultado crudo RAGAS:")
                    print(f"   Tipo: {type(result)}")
                    print(f"   Contenido: {result}")
                    
                    if hasattr(result, 'keys'):
                        print(f"   Keys disponibles: {list(result.keys())}")
                        for key in result.keys():
                            value = result[key]
                            print(f"   {key}: {value} (tipo: {type(value)})")
                    
                except Exception as eval_error:
                    print(f"‚ùå ERROR EN EVALUACI√ìN RAGAS: {eval_error}")
                    import traceback
                    traceback.print_exc()
                    
                    # Crear resultado por defecto para continuar debug
                    result = {
                        "faithfulness": 0.0,
                        "answer_relevancy": 0.0,
                        "context_precision": 0.0,
                        "context_recall": 0.0
                    }
                
                # ‚úÖ SANITIZACI√ìN CON DEBUG
                def sanitize_ragas_value(value):
                    print(f"      üîß Sanitizando: {value} (tipo: {type(value)})")
                    
                    import math
                    import numpy as np
                    
                    original_value = value
                    
                    # Si es lista, tomar primer elemento
                    if isinstance(value, list):
                        if len(value) > 0:
                            value = value[0]
                            print(f"         Lista ‚Üí primer elemento: {value}")
                        else:
                            print(f"         Lista vac√≠a ‚Üí 0.0")
                            return 0.0
                    
                    # Convertir numpy types a Python natives
                    if isinstance(value, np.ndarray):
                        value = float(value.item())
                        print(f"         ndarray ‚Üí float: {value}")
                    elif hasattr(value, 'item'):  # numpy scalar
                        value = float(value.item())
                        print(f"         numpy scalar ‚Üí float: {value}")
                    elif isinstance(value, (np.float64, np.float32, np.int64, np.int32)):
                        value = float(value)
                        print(f"         numpy type ‚Üí float: {value}")
                    
                    # Manejar NaN, inf, -inf
                    if isinstance(value, (int, float)):
                        if math.isnan(value):
                            print(f"         NaN detectado ‚Üí 0.0")
                            return 0.0
                        elif math.isinf(value):
                            print(f"         Inf detectado ‚Üí 0.0")
                            return 0.0
                        else:
                            sanitized = round(float(value), 4)
                            print(f"         Valor v√°lido: {sanitized}")
                            return sanitized
                    
                    # Fallback
                    print(f"         Tipo no reconocido: {type(value)} ‚Üí 0.0")
                    return 0.0
                
                # ‚úÖ PROCESAR CADA M√âTRICA INDIVIDUALMENTE
                print(f"üîß Procesando m√©tricas individuales:")
                
                ragas_results[model_name] = {}
                
                metrics_to_process = ["faithfulness", "answer_relevancy", "context_precision", "context_recall"]
                
                for metric_name in metrics_to_process:
                    try:
                        if metric_name in result:
                            raw_value = result[metric_name]
                            print(f"   üìä {metric_name}: {raw_value} (tipo: {type(raw_value)})")
                            sanitized_value = sanitize_ragas_value(raw_value)
                            ragas_results[model_name][f"ragas_{metric_name}"] = sanitized_value
                            print(f"      ‚úÖ Sanitizado: {sanitized_value}")
                        else:
                            print(f"   ‚ùå {metric_name} no encontrado en resultado")
                            ragas_results[model_name][f"ragas_{metric_name}"] = 0.0
                    except Exception as metric_error:
                        print(f"   ‚ùå Error procesando {metric_name}: {metric_error}")
                        ragas_results[model_name][f"ragas_{metric_name}"] = 0.0
                
                print(f"‚úÖ RAGAS procesado para {model_name}: {ragas_results[model_name]}")
                
            except Exception as e:
                print(f"‚ùå Error general para {model_name}: {e}")
                import traceback
                traceback.print_exc()
                ragas_results[model_name] = {
                    "ragas_faithfulness": 0.0,
                    "ragas_answer_relevancy": 0.0,
                    "ragas_context_precision": 0.0,
                    "ragas_context_recall": 0.0,
                    "ragas_error": str(e)
                }
        
        return ragas_results
        
    except Exception as e:
        print(f"‚ùå Error general en RAGAS: {e}")
        import traceback
        traceback.print_exc()
        return {}

def validate_ragas_inputs(user_query: str, contexts: List[str], answer: str) -> bool:
    """
    Valida que los inputs para RAGAS sean v√°lidos.
    """
    if not user_query or not user_query.strip():
        return False
    if not answer or not answer.strip():
        return False
    if not contexts or len(contexts) == 0:
        return False
    return True