"""
Integraci√≥n de m√©tricas RAGAS para evaluaci√≥n est√°ndar.
RAGAS 0.2.0 con LlamaIndex (sin LangChain)
"""
import logging
import os
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

def calculate_ragas_metrics(user_query, model_responses, contexts, judge_response, config=None):
    """
    Calcula m√©tricas RAGAS usando el MISMO modelo juez seleccionado por el usuario
    RAGAS 0.2.0 + LlamaIndex (sin LangChain)
    """
    try:
        # ‚úÖ CONFIGURAR TIMEOUTS DE RAGAS ANTES DE TODO
        os.environ["RAGAS_EXECUTOR_TIMEOUT"] = "1800"  # 30 minutos
        os.environ["RAGAS_DO_NOT_TRACK"] = "true"
        
        # ‚úÖ USAR EL MISMO PATR√ìN QUE LAS M√âTRICAS NATIVAS
        judge_model_name = config.get("judge_model") if config else None
        llm_url = config.get("llm_url") if config else "http://localhost:11434"
        
        if not judge_model_name:
            print("‚ùå No se proporcion√≥ modelo juez para RAGAS")
            return {}
        
        print(f"üéØ RAGAS usando modelo juez seleccionado: {judge_model_name}")
        
        # ‚úÖ IMPORTS CORRECTOS PARA RAGAS 0.2.0 CON LLAMAINDEX
        try:
            from ragas.llms.base import LlamaIndexLLMWrapper
            from ragas.embeddings.base import LlamaIndexEmbeddingsWrapper
            print("‚úÖ Imports correctos desde .base funcionando")
        except ImportError as e:
            print(f"‚ùå No se pudo importar desde .base: {e}")
            return {}
        
        # ‚úÖ CREAR EL MISMO LLM QUE USAN LAS M√âTRICAS NATIVAS CON TIMEOUT EXTENDIDO
        from llama_index.llms.ollama import Ollama
        judge_llm = Ollama(model=judge_model_name, base_url=llm_url, request_timeout=1800.0)  # 30 minutos
        
        # ‚úÖ CONFIGURAR RAGAS CON ESE LLM (SOLO LLAMAINDEX)
        from ragas import evaluate
        # ‚úÖ IMPORTAR SOLO LAS M√âTRICAS QUE FUNCIONAN
        from ragas.metrics import faithfulness, context_recall  # ‚Üê SOLO ESTAS DOS

        # Usar los wrappers correctos
        ragas_llm = LlamaIndexLLMWrapper(llm=judge_llm)

        # ‚úÖ USAR EL MISMO EMBED_MODEL QUE EL PROYECTO
        from cache_manager import get_cache_manager
        cache_manager = get_cache_manager()
        embed_model = cache_manager.get_cached_embedding_model()
        ragas_embeddings = LlamaIndexEmbeddingsWrapper(embeddings=embed_model)

        # ‚úÖ CONFIGURAR RUNCONFIG PARA RAGAS 0.2.0 SEG√öN DOCUMENTACI√ìN OFICIAL
        from ragas.run_config import RunConfig

        # Crear configuraci√≥n con timeouts extendidos seg√∫n documentaci√≥n
        ragas_run_config = RunConfig(
            timeout=1800,        # 30 minutos (vs 180s por defecto)
            max_retries=3,       # Reintentos en caso de fallo temporal
            max_wait=120,        # M√°ximo 2 minutos entre reintentos
            max_workers=1        # Un solo worker para evitar sobrecarga
        )

        print(f"üîß RunConfig configurado: timeout={ragas_run_config.timeout}s, max_retries={ragas_run_config.max_retries}")

        # ‚úÖ CONFIGURAR SOLO LAS M√âTRICAS QUE FUNCIONAN CON RUNCONFIG
        print(f"üîß === CONFIGURANDO M√âTRICAS RAGAS CON RUNCONFIG ===")

        # Para faithfulness: necesita question, answer, contexts
        try:
            faithfulness.llm = ragas_llm
            # ‚úÖ APLICAR RUNCONFIG A LA M√âTRICA
            if hasattr(faithfulness, 'run_config'):
                faithfulness.run_config = ragas_run_config
                print(f"‚úÖ Faithfulness configurado con RunConfig extendido")
            else:
                print(f"‚ö†Ô∏è Faithfulness no tiene atributo run_config")
        except Exception as e:
            print(f"‚ùå Error configurando faithfulness: {e}")

        # Para context_recall: necesita question, ground_truth, contexts, embeddings
        try:
            context_recall.llm = ragas_llm
            context_recall.embeddings = ragas_embeddings
            # ‚úÖ APLICAR RUNCONFIG A LA M√âTRICA
            if hasattr(context_recall, 'run_config'):
                context_recall.run_config = ragas_run_config
                print(f"‚úÖ Context recall configurado con RunConfig extendido")
            else:
                print(f"‚ö†Ô∏è Context recall no tiene atributo run_config")
        except Exception as e:
            print(f"‚ùå Error configurando context_recall: {e}")

        print(f"‚úÖ RAGAS configurado con juez: {judge_model_name} y RunConfig extendido")
        
        # Calcular m√©tricas
        ragas_results = {}
        
        for model_name, response_text in model_responses.items():
            try:
                from datasets import Dataset
                
                # ‚úÖ DEBUG MEJORADO PARA VERIFICAR RESPUESTA DE REFERENCIA
                print(f"\nüîç === DEBUG COMPLETO PARA {model_name} ===")
                print(f"üìù User query ({len(user_query)} chars): {user_query}")
                print(f"üìù Model response ({len(response_text)} chars): {response_text[:200]}...")
                
                # ‚úÖ DEBUG ESPEC√çFICO DE JUDGE RESPONSE
                if judge_response:
                    print(f"üìù Judge reference ({len(str(judge_response))} chars): {str(judge_response)[:200]}...")
                    print(f"‚úÖ Judge response v√°lido: {len(str(judge_response).strip()) > 20}")
                else:
                    print(f"‚ùå Judge response: NONE - Se usar√° ground truth alternativo")
                
                print(f"üìù Contexts count: {len(contexts)}")
                
                # ‚úÖ GROUND TRUTH CON PREFERENCIA POR RESPUESTA DEL JUEZ
                # ‚úÖ VERIFICAR QUE GROUND TRUTH SEA DIFERENTE DEL ANSWER
                if judge_response and len(str(judge_response).strip()) > 20:
                    judge_text = str(judge_response).strip()
                    
                    # ‚úÖ VERIFICAR QUE NO SEAN ID√âNTICOS
                    if judge_text == response_text:
                        print(f"‚ö†Ô∏è Judge response id√©ntico al model response - usando ground truth alternativo")
                        ground_truth = f"A comprehensive, authoritative answer addressing: {user_query}"
                    else:
                        ground_truth = judge_text
                        print(f"‚úÖ Usando respuesta del juez como ground truth (diferente del modelo)")
                        print(f"üìù Ground truth (juez): {ground_truth[:150]}...")
                else:
                    ground_truth = f"A comprehensive answer addressing: {user_query}"
                    print(f"‚ö†Ô∏è Usando ground truth alternativo")
                    print(f"üìù Ground truth (alternativo): {ground_truth}")
                
                # ‚úÖ OPTIMIZAR DATOS PARA RAGAS - REDUCIR COMPLEJIDAD
                print(f"üîÑ === OPTIMIZANDO DATOS PARA RAGAS (timeout 180s) ===")

                # ‚úÖ LIMITAR CONTEXTOS A M√ÅXIMO 2 FRAGMENTOS M√ÅS CORTOS
                limited_contexts = contexts[:2] if contexts else ["No context available"]
                truncated_contexts = []

                for ctx in limited_contexts:
                    # ‚úÖ TRUNCAR CADA CONTEXTO A M√ÅXIMO 300 CARACTERES
                    if len(ctx) > 300:
                        truncated_ctx = ctx[:300] + "..."
                        truncated_contexts.append(truncated_ctx)
                        print(f"   ‚úÇÔ∏è Contexto truncado: {len(ctx)} ‚Üí {len(truncated_ctx)} chars")
                    else:
                        truncated_contexts.append(ctx)
                        print(f"   ‚úÖ Contexto mantenido: {len(ctx)} chars")

                # ‚úÖ TRUNCAR RESPUESTA DEL MODELO A M√ÅXIMO 200 CARACTERES
                if len(response_text) > 200:
                    truncated_response = response_text[:200] + "..."
                    print(f"   ‚úÇÔ∏è Respuesta truncada: {len(response_text)} ‚Üí {len(truncated_response)} chars")
                else:
                    truncated_response = response_text
                    print(f"   ‚úÖ Respuesta mantenida: {len(response_text)} chars")

                # ‚úÖ TRUNCAR GROUND TRUTH A M√ÅXIMO 150 CARACTERES
                if judge_response and len(str(judge_response).strip()) > 20:
                    judge_text = str(judge_response).strip()
                    if len(judge_text) > 150:
                        ground_truth = judge_text[:150] + "..."
                        print(f"   ‚úÇÔ∏è Ground truth truncado: {len(judge_text)} ‚Üí {len(ground_truth)} chars")
                    else:
                        ground_truth = judge_text
                        print(f"   ‚úÖ Ground truth del juez mantenido: {len(ground_truth)} chars")
                else:
                    ground_truth = f"Answer to: {user_query[:50]}..."
                    print(f"   ‚ö†Ô∏è Ground truth alternativo corto: {len(ground_truth)} chars")

                # ‚úÖ CREAR DATASET OPTIMIZADO
                data = {
                    "question": [user_query[:100]],  # ‚úÖ TRUNCAR PREGUNTA TAMBI√âN
                    "answer": [truncated_response],
                    "contexts": [truncated_contexts],  # ‚úÖ CONTEXTOS LIMITADOS Y TRUNCADOS
                    "ground_truth": [ground_truth]
                }

                print(f"üìä Dataset RAGAS optimizado:")
                print(f"   question: {len(data['question'][0])} chars")
                print(f"   answer: {len(data['answer'][0])} chars") 
                print(f"   contexts: {len(data['contexts'][0])} items, total: {sum(len(c) for c in data['contexts'][0])} chars")
                print(f"   ground_truth: {len(data['ground_truth'][0])} chars")
                print(f"   üéØ Total chars: {sum(len(str(v[0])) for v in data.values())} (objetivo: <800)")
                
                dataset = Dataset.from_dict(data)
                print(f"‚úÖ Dataset HuggingFace creado correctamente")
                
                # ‚úÖ DEBUG PREVIO A EVALUACI√ìN - Verificar configuraci√≥n COMPLETA
                print(f"üîç === VERIFICANDO CONFIGURACI√ìN RAGAS ===")
                print(f"   Judge LLM configurado: {judge_llm}")
                print(f"   Embed model configurado: {embed_model}")
                print(f"   Faithfulness LLM: {getattr(faithfulness, 'llm', 'NO CONFIGURADO')}")
                print(f"   Context recall LLM: {getattr(context_recall, 'llm', 'NO CONFIGURADO')}")
                print(f"   Context recall embeddings: {getattr(context_recall, 'embeddings', 'NO CONFIGURADO')}")
                # ‚úÖ M√âTRICAS DESACTIVADAS:
                print(f"   ‚ö†Ô∏è Answer relevancy: DESACTIVADA (problemas de NaN)")
                print(f"   ‚ö†Ô∏è Context precision: DESACTIVADA (problemas de NaN)")
                
                # ‚úÖ TESTE DE COMPONENTES ANTES DE EVALUACI√ìN
                print(f"üîç === TESTE DE COMPONENTES ===")
                
                # Test modelo juez
                try:
                    test_response = judge_llm.complete("Test simple")
                    print(f"   ‚úÖ Modelo juez responde: {str(test_response)[:50]}...")
                except Exception as test_error:
                    print(f"   ‚ùå Modelo juez no responde: {test_error}")
                
                # Test embeddings
                try:
                    test_embedding = embed_model.get_text_embedding("test embedding")
                    print(f"   ‚úÖ Embeddings funcionan: {len(test_embedding)} dimensiones")
                except Exception as embed_error:
                    print(f"   ‚ùå Embeddings no funcionan: {embed_error}")
                
                # Test ragas wrappers
                try:
                    # ‚úÖ USAR EL M√âTODO CORRECTO DE RAGAS 0.2.0
                    test_prompt = "Test RAGAS wrapper"
                    
                    # El wrapper de RAGAS usa 'generate' en lugar de 'complete'
                    if hasattr(ragas_llm, 'generate'):
                        ragas_test = ragas_llm.generate(test_prompt)
                        print(f"   ‚úÖ RAGAS LLM wrapper funciona: {str(ragas_test)[:50]}...")
                    elif hasattr(ragas_llm, 'complete'):
                        ragas_test = ragas_llm.complete(test_prompt)
                        print(f"   ‚úÖ RAGAS LLM wrapper funciona: {str(ragas_test)[:50]}...")
                    else:
                        # Solo verificar que el wrapper existe
                        print(f"   ‚úÖ RAGAS LLM wrapper creado correctamente: {type(ragas_llm)}")
                        print(f"   üìã M√©todos disponibles: {[m for m in dir(ragas_llm) if not m.startswith('_')]}")
                        
                except Exception as wrapper_error:
                    print(f"   ‚ö†Ô∏è RAGAS LLM wrapper test fall√≥: {wrapper_error}")
                    print(f"   ‚ÑπÔ∏è Esto es normal - el wrapper funciona para evaluate() pero no para test directo")
                
                # ‚úÖ VERIFICAR DATOS DEL DATASET M√ÅS DETALLADAMENTE
                print(f"üîç === VERIFICANDO DATOS DATASET ===")
                print(f"   Question: '{data['question'][0]}'")
                print(f"   Question v√°lida: {len(data['question'][0]) > 5}")
                print(f"   Answer length: {len(data['answer'][0])}")
                print(f"   Answer v√°lida: {len(data['answer'][0]) > 10}")
                print(f"   Contexts count: {len(data['contexts'][0])}")
                print(f"   Contexts v√°lidos: {len(data['contexts'][0]) > 0 and data['contexts'][0] != ['No context available']}")
                print(f"   Ground truth length: {len(data['ground_truth'][0])}")
                print(f"   Ground truth v√°lido: {len(data['ground_truth'][0]) > 10}")
                
                # Mostrar contenido real
                if data['contexts'][0]:
                    print(f"   Primera context preview: {data['contexts'][0][0][:100]}...")
                print(f"   Ground truth preview: {data['ground_truth'][0][:100]}...")
                
                # ‚úÖ EVALUAR UNA M√âTRICA A LA VEZ PARA IDENTIFICAR PROBLEMAS CON TIMEOUT CONFIGURADO
                print(f"üîÑ === EVALUACI√ìN INDIVIDUAL DE M√âTRICAS CON TIMEOUT EXTENDIDO ===")
                
                try:
                    individual_results = {}
                    
                    metrics_to_test = [
                        ("faithfulness", faithfulness),
                        #("answer_relevancy", answer_relevancy), 
                        #("context_precision", context_precision),
                        ("context_recall", context_recall)
                    ]
                    
                    for metric_name, metric_obj in metrics_to_test:
                        try:
                            print(f"   üîÑ Evaluando {metric_name} individualmente...")
                            
                            # Verificar configuraci√≥n espec√≠fica de la m√©trica
                            if hasattr(metric_obj, 'llm'):
                                print(f"      LLM configurado: {metric_obj.llm is not None}")
                            if hasattr(metric_obj, 'embeddings'):
                                print(f"      Embeddings configurado: {metric_obj.embeddings is not None}")
                            
                            # ‚úÖ CONFIGURACI√ìN DE TIMEOUT EN EVALUATE
                            individual_result = evaluate(
                                dataset=dataset,
                                metrics=[metric_obj]
                            )
                            
                            value = individual_result[metric_name]
                            print(f"   ‚úÖ {metric_name}: {value} (tipo: {type(value)})")
                            
                            # Verificar si es NaN
                            import math
                            if isinstance(value, float) and math.isnan(value):
                                print(f"      ‚ö†Ô∏è {metric_name} devolvi√≥ NaN - problema en configuraci√≥n o datos")
                            
                            individual_results[metric_name] = value
                            
                        except Exception as individual_error:
                            print(f"   ‚ùå {metric_name} fall√≥ individualmente: {individual_error}")
                            import traceback
                            print(f"      Traceback: {traceback.format_exc()[:300]}...")
                            individual_results[metric_name] = float('nan')
                    
                    # Usar resultados individuales
                    result = individual_results
                    print(f"‚úÖ Evaluaci√≥n individual completada: {result}")
                    
                except Exception as eval_error:
                    print(f"‚ùå ERROR EN EVALUACI√ìN INDIVIDUAL: {eval_error}")
                    import traceback
                    traceback.print_exc()
                    
                    # Crear resultado por defecto
                    result = {
                        "faithfulness": float('nan'),
                        "answer_relevancy": float('nan'),
                        "context_precision": float('nan'),
                        "context_recall": float('nan')
                    }
                
                # ‚úÖ DEBUG: Inspeccionar resultado crudo
                print(f"üîç Resultado crudo RAGAS:")
                print(f"   Tipo: {type(result)}")
                print(f"   Contenido: {result}")
                
                if hasattr(result, 'keys'):
                    print(f"   Keys disponibles: {list(result.keys())}")
                    for key in result.keys():
                        value = result[key]
                        print(f"   {key}: {value} (tipo: {type(value)})")
                    
                # ‚úÖ SANITIZACI√ìN MEJORADA PARA NaN
                def sanitize_ragas_value(value):
                    print(f"      üîß Sanitizando: {value} (tipo: {type(value)})")
                    
                    import math
                    import numpy as np
                    
                    # ‚úÖ MANEJAR NaN ESPEC√çFICAMENTE PRIMERO
                    if isinstance(value, float) and math.isnan(value):
                        print(f"         ‚ùå NaN detectado - RAGAS no pudo calcular la m√©trica")
                        print(f"         Causas posibles: datos insuficientes, modelo juez no responde, o configuraci√≥n incorrecta")
                        return 0.0
                    
                    # Si es lista, tomar primer elemento
                    if isinstance(value, list):
                        if len(value) > 0:
                            value = value[0]
                            print(f"         Lista ‚Üí primer elemento: {value}")
                            # Verificar NaN en lista
                            if isinstance(value, float) and math.isnan(value):
                                print(f"         ‚ùå NaN en lista")
                                return 0.0
                        else:
                            print(f"         Lista vac√≠a ‚Üí 0.0")
                            return 0.0
                    
                    # Convertir numpy types
                    if isinstance(value, np.ndarray):
                        value = float(value.item())
                        print(f"         ndarray ‚Üí float: {value}")
                    elif hasattr(value, 'item'):
                        value = float(value.item())
                        print(f"         numpy scalar ‚Üí float: {value}")
                    elif isinstance(value, (np.float64, np.float32, np.int64, np.int32)):
                        value = float(value)
                        print(f"         numpy type ‚Üí float: {value}")
                    
                    # Verificar NaN DESPU√âS de conversiones
                    if isinstance(value, (int, float)):
                        if math.isnan(value):
                            print(f"         ‚ùå NaN despu√©s de conversi√≥n")
                            return 0.0
                        elif math.isinf(value):
                            print(f"         ‚ùå Inf detectado ‚Üí 0.0")
                            return 0.0
                        else:
                            sanitized = round(float(value), 4)
                            print(f"         ‚úÖ Valor v√°lido: {sanitized}")
                            return sanitized
                    
                    print(f"         ‚ùå Tipo no reconocido: {type(value)} ‚Üí 0.0")
                    return 0.0
                
                # ‚úÖ PROCESAR CADA M√âTRICA INDIVIDUALMENTE
                print(f"üîß Procesando m√©tricas individuales:")
                
                ragas_results[model_name] = {}
                
                metrics_to_process = ["faithfulness", "context_recall"]  # ‚úÖ SOLO ESTAS DOS
                
                for metric_name in metrics_to_process:
                    try:
                        if metric_name in result:
                            raw_value = result[metric_name]
                            print(f"   üìä {metric_name}: {raw_value} (tipo: {type(raw_value)})")
                            sanitized_value = sanitize_ragas_value(raw_value)
                            ragas_results[model_name][f"ragas_{metric_name}"] = sanitized_value
                            print(f"      ‚úÖ Sanitizado: {sanitized_value}")
                        else:
                            print(f"   ‚ùå {metric_name} no encontrado en resultado")
                            ragas_results[model_name][f"ragas_{metric_name}"] = 0.0
                    except Exception as metric_error:
                        print(f"   ‚ùå Error procesando {metric_name}: {metric_error}")
                        ragas_results[model_name][f"ragas_{metric_name}"] = 0.0
                
                print(f"‚úÖ RAGAS procesado para {model_name}: {ragas_results[model_name]}")
                
            except Exception as e:
                print(f"‚ùå Error general para {model_name}: {e}")
                import traceback
                traceback.print_exc()
                ragas_results[model_name] = {
                    "ragas_faithfulness": 0.0,
                    "ragas_answer_relevancy": 0.0,
                    "ragas_context_precision": 0.0,
                    "ragas_context_recall": 0.0,
                    "ragas_error": str(e)
                }
        
        return ragas_results
        
    except Exception as e:
        print(f"‚ùå Error general en RAGAS: {e}")
        import traceback
        traceback.print_exc()
        return {}

def validate_ragas_inputs(user_query: str, contexts: List[str], answer: str) -> bool:
    """
    Valida que los inputs para RAGAS sean v√°lidos.
    """
    if not user_query or not user_query.strip():
        return False
    if not answer or not answer.strip():
        return False
    if not contexts or len(contexts) == 0:
        return False
    return True